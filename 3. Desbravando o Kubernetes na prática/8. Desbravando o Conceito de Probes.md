# Resumo da Aula sobre Configuração de Sondas (Probes) no Kubernetes e Introdução à Camada de Rede

## Introdução
A aula aborda a configuração de **sondas (probes)** no Kubernetes para verificar a saúde da aplicação `widget-server`, garantindo que ela esteja funcionando corretamente em diferentes estágios do ciclo de vida do pod. As sondas **StartupProbe**, **ReadinessProbe** e **LivenessProbe** são implementadas no arquivo `deployment.yaml`, utilizando a rota `/health` da aplicação. A aula destaca a importância de health checks robustos, especialmente para aplicações com integrações externas (ex.: banco de dados, mensageria), e reforça a natureza **stateless** da aplicação. Por fim, identifica limitações do `kubectl port-forward` para acesso à aplicação e prepara a introdução da camada de rede, com o objeto **Service**, na próxima aula.

## Configuração das Sondas (Probes)
- **Contexto**:
  - A aplicação `widget-server` possui uma rota `/health` simples, adequada para o exemplo, pois não tem integrações externas (ex.: banco de dados, Kafka, cache).
  - **Boa Prática**: Em aplicações reais, o endpoint de health check deve validar integrações críticas (ex.: ping ao banco de dados ou mensageria) para refletir o estado real da aplicação, além de apenas verificar se o servidor está ativo.
  - Exemplo de ferramenta: No ecossistema Node.js, a biblioteca **Terminus** é recomendada para criar health checks que verificam serviços externos, retornando status HTTP 200 se tudo estiver ok.
  - No Kubernetes, três tipos de sondas monitoram diferentes fases do ciclo de vida do pod:
    - **StartupProbe**: Verifica se o container iniciou corretamente.
    - **ReadinessProbe**: Confirma se o pod está pronto para receber tráfego.
    - **LivenessProbe**: Monitora a vivacidade do container em produção, detectando falhas que exigem reinício.

- **Modificações no Arquivo YAML**:
  - As sondas são adicionadas ao `deployment.yaml`, na seção `spec.template.spec.containers`, alinhadas com `ports` e `resources`:
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: widget-server
      namespace: widget
    spec:
      replicas: 5
      selector:
        matchLabels:
          app: widget-server
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 20%
          maxUnavailable: 10%
      template:
        metadata:
          labels:
            app: widget-server
        spec:
          containers:
          - name: widget-server
            image: danielrodrigues/widget-server:v3
            ports:
            - containerPort: 3333
            env:
            - name: SECRET_ACCESS
              value: "#<secret_access_value>"
            - name: BUCKET
              value: "#<bucket_value>"
            - name: ACCOUNT_ID
              value: "#<account_id_value>"
            - name: PUBLIC_URL
              value: "#<public_url_value>"
            - name: LOCALHOST
              value: "http://localhost:3333"
            resources:
              requests:
                cpu: "200m"
                memory: "256Mi"
              limits:
                cpu: "300m"
                memory: "384Mi"
            startupProbe:
              httpGet:
                path: /health
                port: 3333
              failureThreshold: 3
              successThreshold: 1
              timeoutSeconds: 1
              periodSeconds: 15
              initialDelaySeconds: 5
            readinessProbe:
              httpGet:
                path: /health
                port: 3333
              failureThreshold: 3
              successThreshold: 1
              timeoutSeconds: 1
              periodSeconds: 15
              initialDelaySeconds: 15
            livenessProbe:
              httpGet:
                path: /health
                port: 3333
              failureThreshold: 2
              successThreshold: 1
              timeoutSeconds: 1
              periodSeconds: 15
              initialDelaySeconds: 15
    ```

- **Detalhes das Sondas**:
  - **StartupProbe**:
    - **Função**: Garante que o container iniciou corretamente. Se falhar, o container é reiniciado antes de prosseguir.
    - **Configuração**:
      - `httpGet`: Faz uma requisição HTTP GET para `/health` na porta 3333.
      - `failureThreshold: 3`: Após 3 falhas consecutivas, considera o container com problema e o reinicia.
      - `successThreshold: 1`: Um único sucesso indica que o container iniciou corretamente.
      - `timeoutSeconds: 1`: Tempo máximo de espera por resposta (1 segundo, conservador para a rota simples).
      - `periodSeconds: 15`: Executa a verificação a cada 15 segundos.
      - `initialDelaySeconds: 5`: Aguarda 5 segundos após o início do container antes de começar as verificações, evitando falsos negativos durante o bootstrap.
  - **ReadinessProbe**:
    - **Função**: Verifica se o pod está pronto para receber tráfego. Se falhar, o pod é removido do pool de balanceamento de carga (ex.: Service) até estar pronto novamente.
    - **Configuração**:
      - Igual à `StartupProbe`, mas com `initialDelaySeconds: 15` para dar mais tempo de estabilização após a inicialização.
      - **Nota**: Idealmente, deveria usar uma rota `/ready` para verificar prontidão (ex.: inicialização de caches ou conexões), mas `/health` foi usada por simplicidade.
  - **LivenessProbe**:
    - **Função**: Monitora a vivacidade do container em produção. Se falhar, o container é reiniciado automaticamente pelo Kubernetes (mecanismo de **self-healing**).
    - **Configuração**:
      - Similar à `ReadinessProbe`, mas com `failureThreshold: 2` para ser mais rigoroso em produção, reiniciando o container após 2 falhas.
      - `initialDelaySeconds: 15` para evitar verificações prematuras.
    - **Comportamento**: Se `/health` retornar erro (ex.: HTTP 500), o container será reiniciado após 2 falhas consecutivas.

- **Boas Práticas para Sondas**:
  - **Rotas Diferenciadas**: 
    - **Recomendação**: Usar `/ready` para `ReadinessProbe` (verifica inicialização) e `/health` ou `/healthz` para `LivenessProbe` (verifica vivacidade).
    - No exemplo, `/health` foi usada para ambas devido à simplicidade da aplicação, mas em produção, rotas separadas evitam colisões e permitem verificações específicas.
  - **Ajuste de Tempos**:
    - `initialDelaySeconds` deve refletir o tempo de bootstrap da aplicação para evitar loops de falha (ex.: sondas disparando antes da aplicação estar pronta).
    - `periodSeconds` e `timeoutSeconds` devem ser ajustados com base na performance da aplicação. Valores muito baixos podem causar falsos negativos; muito altos, atrasam a detecção de falhas.
  - **Segurança**: 
    - `StartupProbe` e `ReadinessProbe` impedem que uma aplicação com problemas chegue à produção.
    - `LivenessProbe` garante reinícios automáticos em caso de falhas em produção, mas deve ser configurada com cuidado para evitar reinícios desnecessários.

## Aplicação e Validação
- **Aplicação do Deployment**:
  - **Comando**: `kubectl apply -f k8s/deployment.yaml`
    - Aplica o `deployment.yaml` com as sondas configuradas.
  - **Comando**: `watch kubectl get pods -n widget`
    - Monitora a atualização dos pods, que ocorre de forma cadenciada devido à estratégia `RollingUpdate`.
    - Respeita `maxSurge: 20%` (até 6 pods totais) e `maxUnavailable: 10%` (no mínimo 4 pods disponíveis).
    - As sondas garantem que cada novo pod passe pelas verificações de `StartupProbe` e `ReadinessProbe` antes de receber tráfego.
- **Observação da Atualização**:
  - A virada é gradual, com pods antigos sendo terminados e novos sendo criados, respeitando os tempos de `initialDelaySeconds`.
  - A demora observada (devido aos delays de 5s e 15s) é intencional, como mecanismo de segurança para evitar falsos negativos.
  - Em aplicações reais, esses valores podem ser reduzidos se o bootstrap for rápido, ou aumentados se for lento, para evitar loops de reinício.

- **Verificação dos Logs**:
  - **Comando**: `kubectl logs <pod-name> -n widget`
    - Mostra logs da aplicação, incluindo chamadas às sondas a cada 15 segundos (ex.: timestamps como 17:32, 17:47 para `ReadinessProbe` e `LivenessProbe`).
    - As sondas aparecem duplicadas nos logs porque `/health` é usada para ambas (`Readiness` e `Liveness`).
  - **Comando**: `watch kubectl logs <pod-name> -n widget`
    - Permite observar as chamadas em tempo real, confirmando que as sondas estão ativas e disparando a cada 15 segundos.
  - **Comportamento Esperado**:
    - Se `/health` retornar erro (ex.: HTTP 500), a `LivenessProbe` detectará após 2 falhas (30 segundos, devido a `periodSeconds: 15`) e reiniciará o container.
    - Isso ativa o **self-healing** do Kubernetes, recriando o pod para restaurar a saúde.

## Limitações e Observações
- **Acesso à Aplicação**:
  - Cada pod tem um IP dinâmico (ex.: `10.244.2.7`), e o `kubectl port-forward` é limitado a um pod ou controlador (Deployment/ReplicaSet), inadequado para acesso em produção.
  - **Problema**: Clientes não podem acessar a aplicação via `port-forward`, que é apenas uma ferramenta de depuração.
  - **Solução Futura**: O objeto **Service** será introduzido na próxima aula para expor a aplicação com balanceamento de carga, proporcionando um ponto de acesso único e estável.
- **Natureza Stateless**:
  - A aplicação `widget-server` é **stateless**, ideal para pods efêmeros. Para aplicações **stateful**, volumes persistentes serão abordados em aulas futuras.
- **Performance das Sondas**:
  - O tempo de atualização foi ligeiramente lento devido aos valores conservadores de `initialDelaySeconds` (5s para `StartupProbe`, 15s para `ReadinessProbe` e `LivenessProbe`).
  - **Ajuste**: Em produção, testes devem determinar valores otimizados para minimizar latência sem comprometer a estabilidade.
  - **Risco de Loop**: Configurações inadequadas (ex.: `initialDelaySeconds` muito baixo) podem causar loops de reinício, onde a sonda falha antes da aplicação estar pronta, exigindo ajustes via troubleshooting.
- **Logs Descentralizados**:
  - As sondas geram logs em cada pod, visíveis via `kubectl logs`, mas a descentralização dificulta o monitoramento em escala.
  - **Solução Recomendada**: Um sistema de observabilidade centralizado (ex.: ELK, Prometheus) para agregar logs e alertar sobre falhas das sondas.

## Próximos Passos
- **Camada de Rede**:
  - Introduzir o objeto **Service** para resolver o problema de acesso à aplicação, substituindo o `port-forward` por um endpoint estável com balanceamento de carga.
- **Volumes Persistentes**:
  - Configurar armazenamento para aplicações **stateful**, se necessário.
- **Observabilidade**:
  - Implementar ferramentas para centralizar logs e métricas, complementando as sondas.
- **Otimização de Sondas**:
  - Ajustar parâmetros (`periodSeconds`, `initialDelaySeconds`, etc.) com base em testes reais para balancear rapidez e estabilidade.

## Conclusão
A aula configura **StartupProbe**, **ReadinessProbe** e **LivenessProbe** no `deployment.yaml` para monitorar a saúde da aplicação `widget-server`, utilizando a rota `/health`. As sondas garantem que o container inicie corretamente, esteja pronto para tráfego e permaneça saudável em produção, com reinícios automáticos em caso de falhas. A aplicação é atualizada de forma cadenciada, respeitando a estratégia `RollingUpdate` e os tempos das sondas. Limitações como o uso de `port-forward` e logs descentralizados destacam a necessidade de um **Service** e sistemas de observabilidade. A próxima aula abordará a camada de rede, consolidando a exposição da aplicação no Kubernetes.