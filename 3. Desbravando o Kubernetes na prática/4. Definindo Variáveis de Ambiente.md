# Resumo da Aula sobre Atualização da Imagem do Contêiner e Configuração de Recursos no Kubernetes

## Introdução
A aula aborda a correção do erro **404 Not Found** na rota `/health` da aplicação **Widget Server**, identificado na aula anterior. O problema foi causado por uma versão desatualizada da imagem no Docker Hub (`danielrodrigues/widget-server:D49E8862`), que não incluía a rota implementada para o AWS ECS. A solução envolve realizar um novo **build** e **push** da imagem para o Docker Hub com a tag `v3`, atualizar o arquivo `pod.yaml`, e investigar erros adicionais relacionados a variáveis de ambiente e dependências (AWS SDK e New Relic). A aula também destaca a importância de configurar limites de recursos no pod e introduz os conceitos de **ReplicaSet** e **Deployment** para aulas futuras.

## Correção do Problema com a Imagem
- **Contexto do Problema**:
  - A imagem `danielrodrigues/widget-server:D49E8862` no Docker Hub, usada no `pod.yaml`, era antiga (janeiro de 2025, enquanto a aula ocorre em junho de 2025) e não incluía a rota `/health` implementada para o AWS ECS.
  - Anteriormente, os builds eram enviados para o **Amazon ECR** (Elastic Container Registry), que é privado, em vez do Docker Hub. Como o foco da aula é evitar configurações complexas (ex.: **imagePullSecrets** para repositórios privados), a decisão é realizar um build local e enviar a imagem atualizada para o Docker Hub.
- **Passos Executados**:
  - **Build da Imagem**:
    - **Comando**: `docker build -t widget-server:v2 .`
      - Realiza o build da imagem com base no `Dockerfile` no diretório do projeto (`ftr-upload-widget-server`), sem alterações significativas no código.
      - A tag `v2` foi usada inicialmente para testes, mas a aula prossegue com `v3` para evitar conflitos com imagens existentes.
    - Observação: O build foi rápido, pois não havia mudanças no código, aproveitando camadas em cache.
  - **Tag da Imagem**:
    - **Comando**: `docker tag widget-server:v3 danielrodrigues/widget-server:v3`
      - Renomeia a imagem local `widget-server:v3` para o formato do Docker Hub (`danielrodrigues/widget-server:v3`).
  - **Push para o Docker Hub**:
    - **Comando**: `docker push danielrodrigues/widget-server:v3`
      - Envia a imagem para o repositório público no Docker Hub.
      - Requer autenticação com `docker login` para realizar o push, já que o repositório pertence ao usuário `danielrodrigues`.
    - Observação: Como o repositório é público, o Kubernetes pode fazer o **pull** da imagem sem necessidade de credenciais. Para repositórios privados (ex.: ECR), seria necessário configurar um **Secret** no Kubernetes com `imagePullSecrets`.
  - **Atualização do `pod.yaml`**:
    - O arquivo `k8s/pod.yaml` é atualizado para usar a nova tag `v3`:
      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: widget-server
        namespace: widget
      spec:
        containers:
        - name: widget-server
          image: danielrodrigues/widget-server:v3
          ports:
          - containerPort: 3333
          env:
          - name: SECRET_ACCESS
            value: "#<secret_access_value>"
          - name: BUCKET
            value: "#<bucket_value>"
          - name: ACCOUNT_ID
            value: "#<account_id_value>"
          - name: PUBLIC_URL
            value: "#<public_url_value>"
          - name: LOCALHOST
            value: "http://localhost:3333"
      ```
  - **Aplicação e Validação**:
    - **Comando `kubectl apply -f k8s/pod.yaml`**:
      - Reaplica o `pod.yaml` atualizado, recriando o pod com a nova imagem.
    - **Comando `kubectl get pods -n widget`**:
      - Confirma que o pod `widget-server` está no estado **Running**.
    - **Comando `kubectl describe pod widget-server -n widget`**:
      - Mostra eventos do pod, como o **pull** da nova imagem `v3` (já que não existia no nó), a criação do contêiner pelo **Kubelet**, e o início bem-sucedido.
      - O pull levou cerca de 12 segundos, um pouco mais rápido que o anterior (21 segundos), mas ainda considerado alto para um ambiente de produção.

## Novo Erro Identificado
- **Erro nos Logs**:
  - Após atualizar para a imagem `v3`, o pod entra novamente em **CrashLoopBackOff**.
  - **Comando `kubectl logs -n widget widget-server`**:
    - Revela erros relacionados a dependências:
      - **AWS SDK**: A aplicação tenta usar o SDK da AWS, mas as variáveis de ambiente necessárias não estão configuradas corretamente.
      - **New Relic**: A inicialização do agente New Relic falha, possivelmente por falta de configuração (ex.: chave de licença).
      - Variáveis de ambiente relacionadas ao **Zod** (validação) também estão ausentes ou incorretas.
- **Solução Temporária**:
  - Para resolver rapidamente, as linhas de código relacionadas ao AWS SDK e New Relic são comentadas no código-fonte da aplicação (não no `pod.yaml`), já que não são obrigatórias para a execução básica.
  - Observação: Essa abordagem é válida para testes, mas em produção seria uma má prática, pois configurações sensíveis devem ser gerenciadas com **Secrets** ou **ConfigMaps**, e dependências críticas não devem ser removidas.
  - O arquivo `Dockerfile` não foi alterado, então o build reflete apenas as mudanças no código comentado.
- **Novo Build e Push**:
  - **Comando**: `docker build -t widget-server:v3 .`
    - Realiza um novo build com as alterações (AWS SDK e New Relic comentados).
  - **Comando**: `docker tag widget-server:v3 danielrodrigues/widget-server:v3`
  - **Comando**: `docker push danielrodrigues/widget-server:v3`
    - Envia a nova imagem `v3` atualizada para o Docker Hub.
  - O `pod.yaml` já está configurado com a tag `v3`, então não precisa de alterações.
- **Reaplicação e Validação**:
  - **Comando `kubectl apply -f k8s/pod.yaml`**:
    - Recria o pod com a imagem `v3` atualizada.
  - **Comando `kubectl get pods -n widget`**:
    - Confirma que o pod está no estado **Running**.
  - **Comando `kubectl logs -n widget widget-server`**:
    - Mostra a mensagem `HTTP server running`, indicando que a aplicação está funcionando corretamente após comentar as dependências problemáticas.
  - **Comando `kubectl port-forward pod/widget-server -n widget 3333:3333`**:
    - Mapeia a porta local 3333 para a porta 3333 do contêiner.
    - Acessando `http://localhost:3333/health`, a rota agora retorna `message: ok`, confirmando que a nova imagem inclui a rota `/health`.

## Debug e Troubleshooting
- **Importância do Troubleshooting**:
  - A aula enfatiza a relevância de debugar problemas em cenários reais, simulando situações onde a aplicação "funciona na máquina local, mas não no cluster".
  - Ferramentas usadas:
    - **Comando `kubectl logs`**: Para verificar erros específicos no contêiner.
    - **Comando `kubectl describe pod`**: Para analisar eventos do pod, como pull de imagem, criação do contêiner e erros de inicialização.
  - Exemplo prático: Os erros do AWS SDK e New Relic foram identificados nos logs, levando à solução temporária de comentar o código.
- **Lições**:
  - Problemas podem surgir devido a configurações ausentes ou dependências específicas do ambiente (ex.: variáveis de ambiente, chaves de API).
  - Em produção, é essencial versionar imagens corretamente e testar em ambientes de staging para evitar surpresas.

## Configuração de Recursos do Nó
- **Comando `kubectl get nodes`**:
  - Lista os nós do cluster: um **control-plane** e dois **worker** nodes (`worker1` e `worker2`).
- **Comando `kubectl describe node worker2`**:
  - Fornece detalhes sobre o nó onde o pod está rodando:
    - **Capacidade**:
      - **CPU**: 4 vCPUs (virtual CPUs).
      - **Memória RAM**: Quantidade disponível (não especificada em detalhes, mas visível no output).
      - **Máximo de Pods**: 110 pods por nó.
    - **Arquitetura**: `arm64` (confirmado porque o ambiente é um Mac com chip ARM).
    - **IP Interno**: Usado para comunicação dentro do cluster.
  - **Observação sobre Arquitetura**:
    - A imagem `v3` é compatível com `linux/arm64`, alinhada com a arquitetura do nó (`arm64`), evitando problemas de incompatibilidade.
    - Em cenários com arquiteturas diferentes (ex.: AMD64 em um nó ARM64), podem surgir erros, como visto no ECS. É crucial verificar a compatibilidade entre o sistema operacional (OS) e a arquitetura do contêiner e do nó.
- **Problema de Recursos**:
  - O pod não especifica limites de recursos (`resources` no `pod.yaml`), então ele pode consumir toda a CPU e memória disponíveis no nó (`worker2`).
  - Em um ambiente com múltiplos pods, isso leva a uma **disputa de recursos**, causando instabilidade no cluster.
  - O Kubernetes exibe um aviso no `kubectl describe pod` sobre a ausência de limites de recursos, indicando uma má prática.
  - **Solução Futura**: Configurar `resources.requests` e `resources.limits` no `pod.yaml` para definir quotas de CPU e memória, a ser abordado em aulas futuras.

## Próximos Passos
- **Hierarquia de Objetos**:
  - A aula introduz a progressão de objetos no Kubernetes:
    - **Pod**: Menor unidade, já explorada.
    - **ReplicaSet**: Gerencia múltiplas instâncias de um pod para garantir disponibilidade.
    - **Deployment**: Nível mais alto, gerencia ReplicaSets e suporta atualizações contínuas (ex.: rolling updates).
  - Esses conceitos serão explorados nas próximas aulas, com foco em:
    - Configuração de limites de recursos para evitar disputa no nó.
    - Uso de **ReplicaSet** e **Deployment** para escalabilidade e resiliência.
    - Gerenciamento de logs, já que armazenar logs no pod (em memória) não é uma boa prática, devido à sua natureza descartável.
- **Observabilidade**:
  - Logs armazenados localmente no pod são perdidos se o pod for reciclado. Soluções como sistemas centralizados de logs (ex.: ELK, Fluentd) serão necessárias em produção.
- **Imagens Privadas**:
  - A aula menciona que o uso de repositórios privados (ex.: ECR) exige configurações adicionais (`imagePullSecrets`), que serão abordadas posteriormente.

## Conclusão
A aula resolve o erro **404 Not Found** na rota `/health` ao atualizar a imagem do **Widget Server** para a tag `v3` no Docker Hub, após realizar um novo build e comentar dependências problemáticas (AWS SDK e New Relic). O processo de debug é destacado, usando `kubectl logs` e `kubectl describe` para identificar e corrigir erros. A aplicação agora roda corretamente, acessível via `kubectl port-forward`, mas a ausência de limites de recursos no pod é identificada como uma má prática, a ser corrigida em aulas futuras. A aula prepara o terreno para explorar **ReplicaSet** e **Deployment**, além de práticas de observabilidade e gerenciamento de imagens privadas.