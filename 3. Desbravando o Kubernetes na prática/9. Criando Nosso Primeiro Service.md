# Resumo da Aula sobre Configuração de Service no Kubernetes e Introdução à Camada de Rede

## Introdução
A aula introduz a configuração do objeto **Service** no Kubernetes, abordando a camada de rede para proporcionar um ponto de acesso único à aplicação `widget-server`. Até agora, a aplicação foi configurada com **Pods**, **ReplicaSets** e **Deployments**, mas o acesso via `kubectl port-forward` é inadequado para produção devido aos IPs dinâmicos dos pods. O **Service** resolve esse problema ao criar um endpoint estável com balanceamento de carga, distribuindo requisições para os pods com base em seletores. A aula foca no tipo **ClusterIP** (padrão), menciona alternativas como **NodePort** e **LoadBalancer**, e prepara o terreno para tópicos avançados, como escalabilidade horizontal (HPA) e **Secrets**.

## Configuração do Service
- **Contexto**:
  - Cada pod possui uma interface de rede com um IP dinâmico (ex.: `10.244.x.x`), o que torna o acesso direto inviável para clientes.
  - O **Service** atua na camada de rede, na borda do cluster, proporcionando um ponto de acesso único que distribui requisições aos pods correspondentes.
  - **Hierarquia**: 
    - **Deployment**: Gerencia **ReplicaSets** e estratégias de atualização (ex.: RollingUpdate).
    - **ReplicaSet**: Controla o número de réplicas dos **Pods**.
    - **Service**: Não interage com **Deployments** ou **ReplicaSets**, apenas com **Pods**, usando seletores para mapear os IPs dos pods.
  - **Objetivo**: Criar um endpoint estável para a aplicação `widget-server`, substituindo o `port-forward`.

- **Criação do Arquivo `service.yaml`**:
  - Um novo arquivo, `service.yaml`, é criado para definir o **Service**:
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: widget-server-svc
      namespace: widget
    spec:
      selector:
        app: widget-server
      ports:
      - port: 80
        targetPort: 3333
      type: ClusterIP
    ```
  - **Detalhes da Configuração**:
    - **`apiVersion: v1`**: Usada para objetos de rede como **Service**.
    - **`kind: Service`**: Define o tipo de recurso.
    - **`metadata.name: widget-server-svc`**: Nome do serviço, com a convenção `-svc` para indicar que é um **Service**.
    - **`metadata.namespace: widget`**: Aloca o serviço no mesmo namespace do **Deployment** (`widget`).
    - **`spec.selector`**: Usa `app: widget-server` para associar o serviço aos pods com a mesma label definida no `deployment.yaml`.
    - **`spec.ports`**:
      - `port: 80`: Porta exposta pelo serviço (interface externa do cluster).
      - `targetPort: 3333`: Porta do container (definida no `deployment.yaml` como `containerPort: 3333`).
    - **`spec.type: ClusterIP`**: Tipo padrão, cria um IP interno (ex.: `10.x.x.x`) acessível apenas dentro do cluster.

- **Tipos de Service**:
  - **ClusterIP** (padrão):
    - Gera um IP interno para comunicação dentro do cluster.
    - Requer `port-forward` para acesso externo em ambientes de desenvolvimento ou teste.
    - Ideal para serviços internos, combinado com **Ingress** ou gateways em produção.
  - **NodePort**:
    - Expõe o serviço em uma porta alta (ex.: 30000-32767) em cada nó do cluster.
    - Acessível via `<node-ip>:<node-port>`, sem necessidade de `port-forward`.
    - Não recomendado para produção devido à exposição direta e portas não padronizadas, mas útil em testes ou staging.
  - **LoadBalancer**:
    - Cria um balanceador de carga externo (ex.: AWS ELB) com um IP público.
    - Acessível diretamente pela internet, mas caro e geralmente substituído por **Ingress** com **ClusterIP** em produção.
  - **ExternalName**:
    - Mapeia o serviço para um DNS externo, sem criar um IP interno.
    - Pouco comum, não explorado na aula.

- **Funcionamento do Service**:
  - O **Service** usa o seletor (`app: widget-server`) para identificar os pods correspondentes, listando seus IPs como **endpoints**.
  - O Kubernetes gerencia dinamicamente esses endpoints, atualizando-os quando pods são criados ou removidos (ex.: escalabilidade ou falhas).
  - O balanceamento de carga distribui requisições entre os pods listados nos endpoints, garantindo acesso uniforme.

## Aplicação e Validação
- **Criação do Service**:
  - **Erro Inicial**:
    - O comando `kubectl apply -f service.yaml` criou o serviço no namespace `default`, em vez de `widget`.
    - **Correção**: Adicionado `namespace: widget` no `service.yaml` e deletado o serviço incorreto com `kubectl delete svc widget-server-svc -n default`.
  - **Comando**: `kubectl apply -f service.yaml`
    - Cria o serviço `widget-server-svc` no namespace `widget`.
  - **Verificação**:
    - **Comando**: `kubectl get svc -n widget`
      - Exibe o serviço com:
        - Nome: `widget-server-svc`
        - Tipo: `ClusterIP`
        - IP interno: `10.x.x.x`
        - Porta: `80/TCP`
        - Sem IP externo (esperado para `ClusterIP`).
    - **Comando**: `kubectl get svc -n default`
      - Confirma que o namespace `default` não contém o serviço, garantindo a correção.

- **Teste de Acesso**:
  - **Comando**: `kubectl port-forward svc/widget-server-svc -n widget 3333:80`
    - Redireciona a porta local `3333` para a porta `80` do serviço.
    - Permite testar a aplicação via `http://localhost:3333`, com o serviço distribuindo requisições aos pods.
  - **Resultado**:
    - O acesso funciona, confirmando que o serviço redireciona corretamente para os pods na porta `3333` (definida como `targetPort`).
    - Logs (`kubectl logs <pod-name> -n widget`) mostram requisições distribuídas entre os pods, incluindo chamadas das sondas (`/health`) e requisições de teste.
  - **Teste de Falha**:
    - Alterado o seletor no `service.yaml` para `app: api` (valor inválido, não correspondente aos pods).
    - **Comando**: `kubectl apply -f service.yaml`
      - Aplica a mudança, mas o serviço não encontra pods correspondentes.
    - **Comando**: `kubectl describe svc widget-server-svc -n widget`
      - Mostra que o serviço não tem **endpoints**, pois o seletor não faz match com nenhum pod.
    - **Correção**: Revertido o seletor para `app: widget-server` e reaplicado com `kubectl apply`.
      - **Comando**: `kubectl describe svc widget-server-svc -n widget`
        - Confirma que os endpoints foram associados corretamente (ex.: IPs como `10.244.1.23` correspondem aos pods).

- **Exploração de Endpoints**:
  - **Comando**: `kubectl describe svc widget-server-svc -n widget`
    - Exibe os IPs dos pods associados como endpoints (ex.: `10.244.1.23:3333`).
  - **Comando**: `kubectl get endpoints -n widget`
    - Lista os endpoints do serviço, mostrando os IPs e portas dos pods.
    - **Nota**: A estrutura `endpoints` está deprecada (Kubernetes v1.33+), sendo substituída por `EndpointSlice`.
  - **Comando**: `kubectl get endpointslices -n widget`
    - Exibe a mesma informação em um formato mais moderno e escalável, confirmando os IPs dos pods.
  - **Comando**: `kubectl describe pod <pod-name> -n widget`
    - Verifica o IP de um pod específico (ex.: `10.244.1.23`), confirmando que corresponde a um endpoint do serviço.

- **Resiliência do Service**:
  - O **Service** atualiza automaticamente os endpoints quando o número de réplicas muda (ex.: escalabilidade ou falha de pods).
  - Exemplo: Aumentar as réplicas no `deployment.yaml` faz com que novos pods sejam adicionados aos endpoints do serviço sem intervenção manual.

## Limitações e Observações
- **Acesso Externo**:
  - O tipo **ClusterIP** é interno ao cluster, exigindo `port-forward` para acesso local, o que não é viável em produção.
  - **Solução Futura**: Combinar o **Service** com um **Ingress** ou gateway para expor a aplicação publicamente, mantendo o **ClusterIP** como base.
- **Boas Práticas**:
  - **NodePort** deve ser evitado em produção devido à exposição direta e portas não padronizadas.
  - **LoadBalancer** é caro e geralmente substituído por **Ingress** com **ClusterIP** para otimizar custos.
  - Declarar o `namespace` explicitamente no arquivo YAML evita erros de alocação (ex.: criação no `default`).
- **Endpoints e EndpointSlices**:
  - A transição para **EndpointSlice** melhora a escalabilidade em clusters com muitos pods, mas a funcionalidade é semelhante aos **endpoints** tradicionais.
  - O **Service** depende do seletor para associar pods; erros no seletor (ex.: `app: api`) resultam em endpoints vazios, bloqueando o acesso.
- **Controladores vs. Rede**:
  - **Deployment** e **ReplicaSet** são controladores, responsáveis por gerenciar réplicas e atualizações, mas não pela exposição da aplicação.
  - O **Service** opera diretamente com os pods, ignorando os controladores, o que simplifica a camada de rede.
- **Imagens Privadas**:
  - A configuração de containers privados (ex.: usando **ImagePullSecrets**) não foi abordada, mas será tratada em aulas futuras como um tópico avançado.

## Próximos Passos
- **Escalabilidade Horizontal (HPA)**:
  - Configurar **HorizontalPodAutoscaler** para ajustar o número de réplicas com base em métricas como CPU ou tráfego.
- **Secrets**:
  - Explorar a gestão de credenciais e configurações sensíveis com **Secrets**, incluindo **ImagePullSecrets** para imagens privadas.
- **Ingress**:
  - Substituir o `port-forward` por um **Ingress** para exposição pública da aplicação, integrando com o **Service** tipo **ClusterIP**.
- **Outros Recursos**:
  - Testar outras aplicações (ex.: NGINX) com a mesma estrutura de **Pod**, **Deployment** e **Service** para consolidar o aprendizado.

## Conclusão
A aula configura um **Service** tipo **ClusterIP** no arquivo `service.yaml`, criando um ponto de acesso único para a aplicação `widget-server` no namespace `widget`. O serviço usa o seletor `app: widget-server` para mapear os pods, distribuindo requisições via balanceamento de carga. Testes com `port-forward` e comandos como `kubectl describe` confirmam o funcionamento, enquanto erros intencionais (seletor inválido) demonstram a dependência do match entre serviço e pods. A abordagem reforça a separação entre controladores (**Deployment**, **ReplicaSet**) e rede (**Service**), destacando a flexibilidade do Kubernetes. A próxima aula abordará tópicos avançados, como escalabilidade horizontal e **Secrets**, consolidando a gestão de aplicações no cluster.