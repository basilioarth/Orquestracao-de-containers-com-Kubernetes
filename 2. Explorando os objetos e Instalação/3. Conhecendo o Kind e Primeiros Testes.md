# Resumo da Aula sobre Criação do Primeiro Cluster Kubernetes com Kind

## Introdução
A aula foca na criação do primeiro cluster Kubernetes utilizando a ferramenta **Kind** (Kubernetes in Docker), com o objetivo de demonstrar como configurar um cluster localmente e interagir com ele via **kubectl**. O professor reforça a importância do kubectl como ferramenta de interface com a API do Kubernetes e aborda boas práticas, como evitar configurações imperativas e configurar um cluster com separação clara entre **Control Plane** e **Worker Nodes**. A aula também explora os primeiros passos práticos, incluindo a verificação da instalação do kubectl e a criação de um cluster.

## Verificação do Kubectl
- **Comando `kubectl version`**:
  - Usado para verificar se o kubectl está instalado corretamente.
  - Inicialmente, retorna um erro (`connection refused`) se nenhum cluster Kubernetes estiver em execução, o que é esperado em um ambiente sem cluster configurado.
  - Confirma que o kubectl está instalado, mesmo sem um cluster ativo.
- **Comando `kubectl`**:
  - Ao executar apenas `kubectl`, exibe uma lista de subcomandos disponíveis, como `get`, `edit`, `delete`, `run`, `create`, `set`, entre outros.
  - Exemplos de uso:
    - `get`: Lista recursos (ex.: `kubectl get pods` para listar pods).
    - `create` e `run`: Criam recursos de forma imperativa, mas não são recomendados, conforme discutido anteriormente.
  - Reforça a preferência por comandos declarativos (via arquivos YAML) em vez de imperativos.

## Ferramentas para Criar Clusters
- **Kubernetes como Open Source**:
  - Pode ser executado em qualquer ambiente, incluindo máquinas locais, servidores bare metal ou provedores de nuvem (ex.: AWS EKS, Azure AKS).
  - Para ambientes locais, ferramentas como **Kind**, **Minikube**, **KubeADM**, **K3D**, **K3S** e **Rancher** são sugeridas.
- **Kind (Kubernetes in Docker)**:
  - Ferramenta escolhida para a aula, que permite rodar um cluster Kubernetes em contêineres Docker.
  - Ideal para testes locais, pois é leve e fácil de configurar.
  - Requer o **Docker** instalado, já que o Kind usa contêineres para simular nós do cluster.
  - Faz parte do projeto **Kubernetes SIGs** (Special Interest Groups), que inclui plugins e ferramentas para o ecossistema Kubernetes.
- **Outras Ferramentas**:
  - **Minikube**: Alternativa para clusters locais, similar ao Kind.
  - **KubeADM**: Usado para configurações mais avançadas em servidores.
  - **K3D/K3S**: Versões leves do Kubernetes, úteis para ambientes com recursos limitados.
  - **Rancher**: Plataforma para gerenciamento de clusters.
  - **EKS/AKS**: Clusters gerenciados em nuvem, onde o kubectl é necessário, mas ferramentas como Kind não são usadas, pois o provedor gerencia o cluster.

## Criação do Cluster com Kind
- **Pré-requisito**: Docker instalado (já abordado em aulas anteriores sobre contêineres).
- **Comando `kind create cluster`**:
  - Cria um cluster Kubernetes local usando o Kind.
  - Baixa uma imagem de nó (ex.: `kindest/node:v1.33.1`) e configura o cluster com um nó padrão que inclui o **Control Plane**.
  - Gera automaticamente o arquivo **kubeconfig** na pasta `~/.kube/config`, contendo:
    - Informações de autenticação (certificados, chaves, tokens).
    - Endereço do servidor (API do cluster).
    - Contexto do cluster (ex.: `kind-kind`).
- **Saída do Comando**:
  - Mostra o progresso da criação, incluindo:
    - Download da imagem do nó.
    - Inicialização do **Control Plane**.
    - Instalação do **CNI** (Container Network Interface) para rede.
    - Configuração de uma classe de armazenamento padrão (relacionada a volumes, a ser abordada posteriormente).
- **Verificação do Cluster**:
  - Após a criação, o comando `kubectl version` agora funciona sem erros, indicando que o cluster está ativo (ex.: servidor na versão v1.33.1).
  - O comando `kubectl cluster-info` exibe detalhes do cluster, como o **Control Plane** e o **CoreDNS** (parte do CNI) em execução.

## Exploração do Cluster
- **Comando `kubectl get nodes`**:
  - Lista os nós do cluster.
  - Resultado: Apenas um nó (`kind-control-plane`), que atua como **Control Plane**, sem **Worker Nodes** configurados.
  - Problema: O cluster criado possui apenas um nó compartilhado (Control Plane + Worker), o que contraria a boa prática de separação de responsabilidades.
- **Comando `kubectl get pods --all-namespaces`**:
  - Lista pods em todos os namespaces.
  - Exibe pods do namespace `kube-system`, que pertencem ao **Control Plane**, como:
    - **CoreDNS**: Gerencia resolução de nomes na rede.
    - **etcd**: Banco de dados do cluster.
    - **kube-apiserver**: API do Kubernetes.
    - **kube-controller-manager**: Gerencia controladores.
    - **kube-proxy**: Gerencia rede nos nós.
    - **kube-scheduler**: Agenda pods.
    - **local-path-storage**: Relacionado ao Kind para armazenamento.
  - Nenhum pod de aplicação está presente no namespace padrão (`kubectl get pods` retorna "No resources found").
- **Comando `kubectl api-resources`**:
  - Lista todos os recursos disponíveis na API do Kubernetes, incluindo **Kind** (tipo de recurso) e **API Version**.
  - Útil para explorar objetos como pods, deployments, services, além de CRDs que expandem a API.

## Problemas Identificados
- **Nó Compartilhado**:
  - O cluster criado tem apenas um nó, que mistura **Control Plane** e **Worker Node**, violando a boa prática de separação.
  - Aplicações criadas nesse nó seriam executadas no **Control Plane**, o que pode comprometer a estabilidade e segurança.
- **Solução**: Configurar o cluster para incluir **Worker Nodes** separados, a ser abordado na próxima aula.

## Conclusão
A aula demonstra a criação de um cluster Kubernetes local usando o **Kind**, destacando a facilidade de configurar um ambiente de teste com o **kubectl** e o **Docker**. O cluster inicial possui apenas um nó compartilhado, o que não segue as boas práticas, mas serve como ponto de partida. Comandos como `kubectl get nodes`, `kubectl get pods --all-namespaces` e `kubectl api-resources` ajudam a explorar o cluster, enquanto o **kubeconfig** facilita a autenticação. A aula prepara para a próxima sessão, que corrigirá a configuração do cluster para incluir **Worker Nodes** e avançará na interação prática com a API do Kubernetes.